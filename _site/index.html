<!DOCTYPE html>
<html>
<head>
  <meta charset='utf-8'>
  <title>Tracey Jaquith - PoohBot Pictures</title>
  <!--
  <link rel="icon" href="favicon.ico" type="image/x-icon">

  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <meta name='author' content=''>
  <meta name='description' content=''>
  <meta name='keywords' content=''>

  <meta name="twitter:title" content="Tracey Jaquith - PoohBot Pictures"/>
  <meta name="twitter:card" content="summary"/>
  <meta name="twitter:description" content=""/>
  <meta name="twitter:site" content="@tracey_pooh"/>

  <meta property="og:title" content="Tracey Jaquith - PoohBot Pictures">
  <meta property="og:image" content="img/traceymonet.jpg">
  <meta property="og:description" content="">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://xxx" />

  -->

  <!--
[layout]
posts
img

pages
css
js


- 15y, 112 posts, 500k txt, 60k gzip (twitter tp cold load 2.7MB)
- xxx SEO!?

- fill out nav & sidebar
- parse 10 most recent posts, fill out main page
- parse front-matter from remaining posts for tag cloud, category list (fills-in later) (cache)
  - consider range requests for ~just front-matter retrieval & parsing?
  - GH pages serves over http2 at least

- put into local storage (cache <= 24h): each post's: title, date, tags, cats, img
  - ... but we only need to parse most recent 2-10 posts from each subscriber
- top page: show most recent 10 posts' summaries
  - page 2: posts 10..20, etc.
- open followers' websites /posts dir to parse their 2-10 most recent .md...
  - eg: https://api.github.com/repos/ajaquith/securitymetrics/contents/source/_posts

  xxx posts w/o dates examples
  - eg: https://api.github.com/repos/stefma/hugo-fresh/contents/exampleSite/content/blog
        https://github.com/StefMa/hugo-fresh/tree/master/exampleSite/content/blog

  -->
  <link href="https://esm.archive.org/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <style>
body {
  font-family: Arial, Helvetica, sans-serif;
  display: grid;
  grid-gap: 10px;
  grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
  align-items: top;
  justify-items: center;
}
#spa {
  display: none;
}
.card {
  display: initial;
  overflow: hidden;
  padding: 5px;
  margin: 10px;
  width: 300px;
}
.card h2 {
  font-size: 22px;
  margin-top: 5px;
}
.card a {
  text-decoration: none;
}
.card img {
  object-fit: cover;
  width: 300px;
  height: 170px;
  margin-top: -5px;
  margin-left: -5px;
  margin-right: -5px;
}
  </style>
</head>


<body>
  <div id="spa"></div>
</body>


<script type="module">
import yml from 'https://esm.archive.org/js-yaml'

const log = console.log.bind(console)

const config = await (await fetch('config.json')).json()
const { img_site, user, repo, posts_per_page } = config

const tag = (decodeURIComponent(location.search).match(/^\?tags\/([^&]+)/) || ['', ''])[1]

const posts = {}

let mds = [] // xxx cache
const tmp = await fetch(`./posts`)
if (tmp.ok) {
  // local dev or something that replies with a directory listing (yay)
  const dir = await (tmp).text()
  mds = [...dir.matchAll(/<a href="([^"]+.md)"/g)].map((e) => e[1])
} else {
  // use GitHub API to get a list of the posts
  const json = await (await fetch(`https://api.github.com/repos/${user}/${repo}/contents/_site/posts`)).json()
  mds = json.map((e) => e.name)
}
const latest = mds.filter((e) => e !== 'README.md' && !e.match(/\/README.md$/)).sort().reverse()
log(latest.slice(0, posts_per_page))

let proms = []
for (let n = 0; n < latest.length; n++) {
  const md = latest[n]
  const url = md.match(/\//) ? md : `./posts/${md}`

  proms.push(fetch(url))

  if (((n + 1) % posts_per_page) && n < latest.length - 1)
    continue // keep building up requests

  // now make the requests in parallel and wait for them all to answer
  const markdowns = await Promise.all(proms)

  for (const markdown of markdowns) {
    const yaml = await markdown.text()

    const front_matter = yaml.split('\n---').shift()
    const json = yml.load(front_matter)
    log({ json })

    const title    = json?.title?.trim() ?? ''
    const tags     = (json?.tags ?? []).map((e) => e.trim().replace(/ /g, '-'))
    const date     = json?.date ?? new Date()
    const featured = json?.featured?.trim() ?? ''
    const year = parseInt(date.getUTCFullYear(), 10) // chexxx UTC
    log({ title, tags, date, featured })
    // author|categories

    if (tag.length && !(tags.includes(tag))) continue

    const img = featured === ''
      ? img_site
      : (featured.match(/\//) ? featured : `./img/${featured}`)

    const taglinks = tags.map((e) => `<a href="./?tags/${e}">${e}</a>`).join(' ')

    posts[date.getTime()] = `
      <div class="card card-body bg-light">
        <img src="${img}">
        <h2>${title}</h2>
        ${`${date}`.split(' ').slice(0, 4).join(' ')}<br>
        Tags: ${taglinks}
      </div>`
  }

  proms = []
}

// document.getElementsByTagName('meta')['keywords'].content = 'updated keywords xxx'
// document.getElementsByTagName('meta')['description'].content = 'updated description xxx'


let str = ''
for (const ymd of Object.keys(posts).sort().reverse())
  str += posts[ymd]
document.getElementById('spa').insertAdjacentHTML('afterend', str)
</script>
